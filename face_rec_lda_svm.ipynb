{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded5946-869c-4252-a09f-31506f44bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd490e-e408-48dc-bfa9-db2b35c45378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "\n",
    "faces = fetch_olivetti_faces()\n",
    "_, img_height, img_width = faces.images.shape\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846566b-1b83-46f6-ac4c-4be9ea5f7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "N_IDENTITIES = len(np.unique(faces.target)) # how many different individuals are in the dataset\n",
    "GALLERY_SIZE = 5                            # use the first GALLERY_SIZE images per individual for training, the rest for testing\n",
    "\n",
    "gallery_indices = []\n",
    "probe_indices = []\n",
    "for i in range(N_IDENTITIES):\n",
    "    indices = list(np.where(faces.target == i)[0])\n",
    "    gallery_indices += indices[:GALLERY_SIZE]\n",
    "    probe_indices += indices[GALLERY_SIZE:]\n",
    "\n",
    "x_train = faces.images[gallery_indices].reshape(-1, img_height*img_width) # vectorize train images\n",
    "y_train = faces.target[gallery_indices]\n",
    "x_test = faces.images[probe_indices].reshape(-1, img_height*img_width)    # vectorize test images\n",
    "y_test = faces.target[probe_indices]\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af251845-23df-421a-8519-09f28918b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image sets\n",
    "def show_images(imgs, num_rows, num_cols):\n",
    "    assert len(imgs) == num_rows*num_cols\n",
    "\n",
    "    full = None\n",
    "    for i in range(num_rows):\n",
    "        row = None\n",
    "        for j in range(num_cols):\n",
    "            if row is None:\n",
    "                row = imgs[i*num_cols+j].reshape(img_height, img_width)*255.0\n",
    "            else:\n",
    "                row = np.concatenate((row, imgs[i*num_cols+j].reshape(img_height, img_width)*255.0), axis=1)\n",
    "        if full is None:\n",
    "            full = row\n",
    "        else:\n",
    "            full = np.concatenate((full, row), axis=0)\n",
    "\n",
    "    f = plt.figure(figsize=(num_cols, num_rows))\n",
    "    plt.imshow(full, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print('TRAINING')\n",
    "show_images(x_train, N_IDENTITIES, GALLERY_SIZE)\n",
    "print('TESTING')\n",
    "show_images(x_test, N_IDENTITIES, 10 - GALLERY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc1b01-f61a-4d2c-bd9d-88e9dad68493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image sets\n",
    "def show_images(imgs, num_rows, num_cols):\n",
    "    assert len(imgs) == num_rows*num_cols\n",
    "\n",
    "    full = None\n",
    "    for i in range(num_rows):\n",
    "        row = None\n",
    "        for j in range(num_cols):\n",
    "            if row is None:\n",
    "                row = imgs[i*num_cols+j].reshape(img_height, img_width)*255.0\n",
    "            else:\n",
    "                row = np.concatenate((row, imgs[i*num_cols+j].reshape(img_height, img_width)*255.0), axis=1)\n",
    "        if full is None:\n",
    "            full = row\n",
    "        else:\n",
    "            full = np.concatenate((full, row), axis=0)\n",
    "\n",
    "    f = plt.figure(figsize=(num_cols, num_rows))\n",
    "    plt.imshow(full, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print('TRAINING')\n",
    "show_images(x_train, N_IDENTITIES, GALLERY_SIZE)\n",
    "print('TESTING')\n",
    "show_images(x_test, N_IDENTITIES, 10 - GALLERY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49cae8-12bf-4657-ba33-4a4e92c5dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "lda = LDA()\n",
    "lda.fit(x_train, y_train)\n",
    "y_pred_lda = lda.predict(x_test)\n",
    "fscore_lda = f1_score(y_test, y_pred_lda, average='macro')\n",
    "cm_lda = confusion_matrix(y_test, y_pred_lda)\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred_svm = svm.predict(x_test)\n",
    "fscore_svm = f1_score(y_test, y_pred_svm, average='macro')\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "# 2D Visualization using LDA\n",
    "lda_2d = LDA(n_components=2)\n",
    "x_lda_2d = lda_2d.fit_transform(x_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for class_id in np.unique(y_train):\n",
    "    plt.scatter(\n",
    "        x_lda_2d[y_train == class_id, 0],\n",
    "        x_lda_2d[y_train == class_id, 1],\n",
    "        label=f\"Person {class_id}\", alpha=0.7, s=30\n",
    "    )\n",
    "    \n",
    "plt.title(\"2D LDA Projection of Olivetti Faces\")\n",
    "plt.xlabel(\"LD1\")\n",
    "plt.ylabel(\"LD2\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small', ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437503a-4ecc-4eb1-9e2e-0137c38d1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average F-scores of LDA and SVM\n",
    "print(f\"Average F1-Score (LDA): {fscore_lda:.4f}\")\n",
    "print(f\"Average F1-Score (SVM): {fscore_svm:.4f}\")\n",
    "\n",
    "# Confusion matrix for LDA\n",
    "fig, ax = plt.subplots(figsize=(8, 8))  # Setting figure size here\n",
    "cm_lda = confusion_matrix(y_test, y_pred_lda)\n",
    "disp_lda = ConfusionMatrixDisplay(cm_lda)\n",
    "disp_lda.plot(cmap='Blues', values_format='d', ax=ax)  \n",
    "plt.title(\"LDA Confusion Matrix\")\n",
    "plt.xticks(fontsize=8, rotation=45)   # 45 degree rotation on x-axis\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for SVM\n",
    "fig, ax = plt.subplots(figsize=(8, 8))   # Setting figure size here\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "disp_svm = ConfusionMatrixDisplay(cm_svm)\n",
    "disp_svm.plot(cmap='Greens', values_format='d', ax=ax)\n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.xticks(fontsize=8, rotation=45)   # 45 degree rotation on x-axis\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32014453-2b94-4965-a400-e3c96b10d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the most confused pair in LDA confusion matrix\n",
    "cm_copy = cm_lda.copy()\n",
    "np.fill_diagonal(cm_copy, 0)  # ignore correct predictions\n",
    "\n",
    "# Number of top confused pairs to display\n",
    "N = 2  # can pass N = 1 to identify the top most confused pair that will be Person 2 <-> Person 22\n",
    "\n",
    "# Get the top-N most confused pairs (flattened indices)\n",
    "flat = cm_copy.flatten()\n",
    "top_n_indices = flat.argsort()[-N:][::-1]\n",
    "\n",
    "for idx in top_n_indices:\n",
    "    i, j = divmod(idx, cm_copy.shape[1])\n",
    "    print(f\"\\n f{N} Most confused pair: Person {i} <-> Person {j} | Count: {cm_copy[i, j]}\")\n",
    "\n",
    "    # Collect test images for Person i and j\n",
    "    imgs = []\n",
    "    for person in [i, j]:\n",
    "        indices = np.where(y_test == person)[0]\n",
    "        imgs.extend(x_test[indices[:5]])  # take first 5 test images\n",
    "\n",
    "    # Visualize the 2x5 grid\n",
    "    show_images(imgs, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5ffd4-52b0-4e82-b176-7ea464cfaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the most confused pair in SVM confusion matrix\n",
    "cm_copy_svm = cm_svm.copy()\n",
    "np.fill_diagonal(cm_copy_svm, 0)  # ignore correct predictions\n",
    "\n",
    "# Number of top confused pairs to display\n",
    "N = 2  # can pass N = 1 to identify the top most confused pair that will be Person 2 <-> Person 22\n",
    "\n",
    "# Get the top-N most confused pairs (flattened indices)\n",
    "flat_svm = cm_copy_svm.flatten()\n",
    "top_n_indices_svm = flat_svm.argsort()[-N:][::-1]\n",
    "\n",
    "for idx in top_n_indices_svm:\n",
    "    i, j = divmod(idx, cm_copy_svm.shape[1])\n",
    "    print(f\"\\n f{N} Most confused pair: Person {i} <-> Person {j} | Count: {cm_copy_svm[i, j]}\")\n",
    "\n",
    "    # Collect test images for Person i and j\n",
    "    imgs_svm = []\n",
    "    for person in [i, j]:\n",
    "        indices = np.where(y_test == person)[0]\n",
    "        imgs_svm.extend(x_test[indices[:5]])  # take first 5 test images\n",
    "\n",
    "    # Visualize the 2x5 grid\n",
    "    show_images(imgs_svm, 2, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
